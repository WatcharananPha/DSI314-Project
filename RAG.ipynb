{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kax8gwzp2149"
   },
   "outputs": [],
   "source": [
    "# requirements = [\n",
    "#     \"langchain\",\n",
    "#     \"langchain-community\",\n",
    "#     \"llama-parse\",\n",
    "#     \"fastembed\",\n",
    "#     \"python-dotenv\",\n",
    "#     \"langchain-groq\",\n",
    "#     \"chainlit\",\n",
    "#     \"sentence-transformers\",\n",
    "#     \"openai\",\n",
    "#     \"langchain-openai\",\n",
    "#     \"nltk\",\n",
    "#     \"joblib\",\n",
    "#     \"gdown\",\n",
    "#     \"PyPDF2\",\n",
    "#     \"faiss-cpu\",\n",
    "#     \"nest-asyncio\",\n",
    "#     \"unstructured[md]\"\n",
    "# ]\n",
    "\n",
    "# file_path = \"requirements.txt\"\n",
    "# with open(file_path, \"w\") as f:\n",
    "#     for package in requirements:\n",
    "#         f.write(f\"{package}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A3ivk3CKTUXs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List, Dict\n",
    "from pypdf import PdfReader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from openai import OpenAI\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from collections import deque\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "YUXHlcBhR7YK"
   },
   "outputs": [],
   "source": [
    "# import gdown\n",
    "# from PyPDF2 import PdfReader\n",
    "\n",
    "# file_ids = [\n",
    "#     '1ohQ7aQCiY4pKqkKl0_FssgYkkHmeYDWN',\n",
    "#     '1SjDi9aY8_jQtDfe5wix-aFYuS5TP0pDG'\n",
    "# ]\n",
    "\n",
    "# for file_id in file_ids:\n",
    "#     file_url = f'https://drive.googlea.com/uc?id={file_id}'\n",
    "#     output_pdf = f'/content/{file_id}.pdf'\n",
    "#     gdown.download(file_url, output_pdf, quiet=False)\n",
    "#     reader = PdfReader(output_pdf)\n",
    "\n",
    "#     for page_num in range(len(reader.pages)):\n",
    "#         page = reader.pages[page_num]\n",
    "#         print(f\"Text from page {page_num + 1}:\\n{page.extract_text()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "iJh3in61TBo_"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "PARSED_DATA_FILE = os.path.join(DATA_DIR, \"parsed_data.pkl\")\n",
    "PDF_FILE = [\n",
    "    'DatasourcePDF/Merged_SplitDocument.pdf'\n",
    "]\n",
    "\n",
    "def extract_text_from_pdf(pdf_file: str) -> str:\n",
    "    reader = PdfReader(pdf_file)\n",
    "    all_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        all_text += page.extract_text()\n",
    "    return all_text\n",
    "\n",
    "def create_vector_database(\n",
    "    llamaparse_api_key: str,\n",
    "    pdf_files: list = PDF_FILE,\n",
    "    data_file: str = PARSED_DATA_FILE,\n",
    ") -> Tuple:\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    all_text = \"\"\n",
    "    for pdf_file in pdf_files:\n",
    "        all_text += extract_text_from_pdf(pdf_file)\n",
    "\n",
    "    text_output = os.path.join(DATA_DIR, \"extracted_text.txt\")\n",
    "    with open(text_output, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=128)\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\") #BAAI/bge-base-en-v1.5\n",
    "    vector_store = FAISS.from_documents(documents=documents, embedding=embed_model)\n",
    "    \n",
    "    faiss_index_path = os.path.join(DATA_DIR, \"faiss_index\")\n",
    "    vector_store.save_local(faiss_index_path)\n",
    "\n",
    "    return vector_store, embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGfAW41WW-SQ"
   },
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key='sk-GqA4Uj6iZXaykbOzIlFGtmdJr6VqiX94NhhjPZaf81kylRzh',\n",
    "    base_url='https://api.opentyphoon.ai/v1'\n",
    ")\n",
    "\n",
    "def summarize_text(text, max_tokens=5000):\n",
    "    tokens = text.split()\n",
    "    return ' '.join(tokens[:max_tokens]) + '...' if len(tokens) > max_tokens else text\n",
    "\n",
    "class ConversationalMemory:\n",
    "    def __init__(self, max_length=10):\n",
    "        self.history = deque(maxlen=max_length)\n",
    "    def add_to_memory(self, question: str, response: str | None):\n",
    "        if response is not None :\n",
    "            self.history.append({\"question\": question, \"response\": response})\n",
    "    def get_memory(self) -> List[Dict[str, str]]:\n",
    "        return list(self.history)\n",
    "    def save_memory_to_file(self, file_path: str):\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.get_memory(), f, indent=4)\n",
    "    def load_memory_from_file(self, file_path: str):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.history = deque(json.load(f), maxlen=self.history.maxlen)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No existing memory file found at {file_path}. Starting fresh.\")\n",
    "\n",
    "def generate_response(prompt):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"typhoon-v1.5x-70b-instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def retrieve_documents(query, retriever):\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "def ask_question_with_memory(retriever, question, memory: ConversationalMemory):\n",
    "    retrieved_docs = retrieve_documents(question, retriever)\n",
    "    summarized_data = summarize_text(\"\\n\".join([doc.page_content for doc in retrieved_docs]), max_tokens=5000)\n",
    "    history_context = \"\\n\".join(\n",
    "        [f\"Q: {entry['question']}\\nA: {entry['response']}\" for entry in memory.get_memory()]\n",
    "    )\n",
    "    full_prompt = (\n",
    "        f\"Conversation history:\\n{history_context}\\n\\n\"\n",
    "        f\"Context for Pathum Thani development:\\n{summarized_data}\\n\\n\"\n",
    "        f\"New question: {question}\"\n",
    "    )\n",
    "    response = generate_response(full_prompt)\n",
    "    memory.add_to_memory(question, response)\n",
    "\n",
    "    return response\n",
    "\n",
    "llamaparse_api_key = \"llx-pNes5rGZru1FvO1nINQMrAJMEso0OEWutgy8ejbGntSxNPeq\"\n",
    "vector_db, embed_model = create_vector_database(llamaparse_api_key)\n",
    "\n",
    "retriever = vector_db.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mvector_db\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector_db' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ll708H0LF22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing memory file found at conversation_memory.json. Starting fresh.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     response \u001b[38;5;241m=\u001b[39m ask_question_with_memory(\u001b[43mretriever\u001b[49m, question, memory)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_memory_to_file(memory_file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    memory = ConversationalMemory(max_length=10)\n",
    "    memory_file = \"conversation_memory.json\"\n",
    "    memory.load_memory_from_file(memory_file)\n",
    "\n",
    "    while True:\n",
    "        question = input(\"Enter your question: \")\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        response = ask_question_with_memory(retriever, question, memory)\n",
    "        print(f\"Answer: {response}\")\n",
    "    memory.save_memory_to_file(memory_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
