{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kongl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "from enum import Enum\n",
    "from collections import deque\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import nltk\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from llama_parse import LlamaParse\n",
    "from openai import OpenAI\n",
    "\n",
    "nltk.download('punkt')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "PARSED_DATA_FILE = os.path.join(DATA_DIR, \"parsed_data.pkl\")\n",
    "PDF_FILE = r\"/content/file_subset.pdf\"\n",
    "PARSING_INSTRUCTIONS = \"\"\"\n",
    "The provided document is a statistical report from the National Statistical Office of Thailand.\n",
    "It contains information about various industries, including employment and revenue.\n",
    "The report is in the Thai language.\n",
    "The document is structured with tables and text sections.\n",
    "Try to extract information accurately and answer questions concisely.\n",
    "\"\"\"\n",
    "\n",
    "class Language(Enum):\n",
    "    THAI = \"th\"\n",
    "    ENGLISH = \"en\"\n",
    "\n",
    "def load_or_parse_data(data_file: str, pdf_file: str, parsing_instructions: str,\n",
    "                      llamaparse_api_key: str, language: Language = Language.THAI) -> List:\n",
    "    if os.path.exists(data_file):\n",
    "        return joblib.load(data_file)\n",
    "\n",
    "    try:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llamaparse_api_key,\n",
    "            result_type=\"markdown\",\n",
    "            parsing_instruction=parsing_instructions,\n",
    "            max_timeout=5000,\n",
    "            language=language.value,\n",
    "        )\n",
    "        parsed_data = parser.load_data(pdf_file)\n",
    "        if not parsed_data:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "    joblib.dump(parsed_data, data_file)\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a vector database\n",
    "def create_vector_database(llamaparse_api_key: str, pdf_file: str = PDF_FILE, data_file: str = PARSED_DATA_FILE) -> Tuple:\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    parsed_documents = load_or_parse_data(\n",
    "        data_file=data_file,\n",
    "        pdf_file=pdf_file,\n",
    "        parsing_instructions=PARSING_INSTRUCTIONS,\n",
    "        llamaparse_api_key=llamaparse_api_key\n",
    "    )\n",
    "\n",
    "    markdown_output = os.path.join(DATA_DIR, \"output.md\")\n",
    "    with open(markdown_output, 'w') as f:\n",
    "        for doc in parsed_documents:\n",
    "            f.write(doc.text + '\\n')\n",
    "\n",
    "    loader = UnstructuredMarkdownLoader(markdown_output)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "    doc_chunks = text_splitter.split_documents(documents)\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    vector_store = FAISS.from_documents(documents=doc_chunks, embedding=embed_model)\n",
    "    faiss_index_path = os.path.join(DATA_DIR, \"faiss_index\")\n",
    "    vector_store.save_local(faiss_index_path)\n",
    "\n",
    "    return vector_store, embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational memory class\n",
    "class ConversationalMemory:\n",
    "    def __init__(self, max_length=10):\n",
    "        self.history = deque(maxlen=max_length)\n",
    "\n",
    "    def add_to_memory(self, question: str, response: str | None):\n",
    "        if response is not None:\n",
    "            self.history.append({\"question\": question, \"response\": response})\n",
    "\n",
    "    def get_memory(self) -> List[Dict[str, str]]:\n",
    "        return list(self.history)\n",
    "\n",
    "    def save_memory_to_file(self, file_path: str):\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.get_memory(), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load_memory_from_file(self, file_path: str):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.history = deque(json.load(f), maxlen=self.history.maxlen)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No existing memory file found at {file_path}. Starting fresh.\")\n",
    "\n",
    "def summarize_text(text, max_tokens=3000):\n",
    "    tokens = text.split()\n",
    "    return ' '.join(tokens[:max_tokens]) + '...' if len(tokens) > max_tokens else text\n",
    "\n",
    "def generate_response(prompt, client):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"typhoon-v1.5x-70b-instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def retrieve_documents(query, retriever):\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "def ask_question(retriever, question, client):\n",
    "    retrieved_docs = retrieve_documents(question, retriever)\n",
    "    summarized_data = summarize_text(\"\\n\".join([doc.page_content for doc in retrieved_docs]), max_tokens=3000)\n",
    "    prompt = f\"Based on the following information: {summarized_data}, answer this question: {question}\"\n",
    "    return generate_response(prompt, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m llamaparse_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllx-bMf1NAZ0TS6EgfsYfXAZADVHk9VHwx79fdoU6E3pwkzBFRqD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m vector_db, embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vector_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllamaparse_api_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mcreate_vector_database\u001b[1;34m(llamaparse_api_key, pdf_file, data_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m doc_chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m     20\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAAI/bge-m3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m faiss_index_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m vector_store\u001b[38;5;241m.\u001b[39msave_local(faiss_index_path)\n",
      "File \u001b[1;32mc:\\Users\\kongl\\Documents\\DSI314 Progress 2\\DSI314\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:852\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    850\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kongl\\Documents\\DSI314 Progress 2\\DSI314\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1042\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m-> 1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kongl\\Documents\\DSI314 Progress 2\\DSI314\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:999\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[1;32m--> 999\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[0;32m   1000\u001b[0m docstore \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore\u001b[39m\u001b[38;5;124m\"\u001b[39m, InMemoryDocstore())\n\u001b[0;32m   1001\u001b[0m index_to_docstore_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "llamaparse_api_key = \"llx-bMf1NAZ0TS6EgfsYfXAZADVHk9VHwx79fdoU6E3pwkzBFRqD\"\n",
    "vector_db, embedding_model = create_vector_database(llamaparse_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    client = OpenAI(\n",
    "        api_key='sk-GqA4Uj6iZXaykbOzIlFGtmdJr6VqiX94NhhjPZaf81kylRzh',\n",
    "        base_url='https://api.opentyphoon.ai/v1'\n",
    "    )\n",
    "    retriever = vector_db.as_retriever(search_kwargs={'k': 10})\n",
    "    question = input(\"Enter your question: \")\n",
    "    response = ask_question(retriever, question, client)\n",
    "\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DSI314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
